{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X..language</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.22470</td>\n",
       "      <td>-5.84695</td>\n",
       "      <td>1.579330</td>\n",
       "      <td>11.07320</td>\n",
       "      <td>-6.74006</td>\n",
       "      <td>13.6003</td>\n",
       "      <td>-8.34855</td>\n",
       "      <td>4.25603</td>\n",
       "      <td>-1.076250</td>\n",
       "      <td>-7.46205</td>\n",
       "      <td>4.02494</td>\n",
       "      <td>-0.636634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4.89987</td>\n",
       "      <td>-4.39598</td>\n",
       "      <td>0.450237</td>\n",
       "      <td>6.70707</td>\n",
       "      <td>-6.72603</td>\n",
       "      <td>10.3793</td>\n",
       "      <td>-9.64491</td>\n",
       "      <td>4.25752</td>\n",
       "      <td>-0.784425</td>\n",
       "      <td>-5.47746</td>\n",
       "      <td>2.21494</td>\n",
       "      <td>-7.412240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.91494</td>\n",
       "      <td>-4.19126</td>\n",
       "      <td>3.497830</td>\n",
       "      <td>10.37480</td>\n",
       "      <td>-6.31339</td>\n",
       "      <td>10.0329</td>\n",
       "      <td>-10.61860</td>\n",
       "      <td>4.82322</td>\n",
       "      <td>1.174930</td>\n",
       "      <td>-3.97906</td>\n",
       "      <td>1.87674</td>\n",
       "      <td>-4.466670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4.88026</td>\n",
       "      <td>-6.57269</td>\n",
       "      <td>2.147220</td>\n",
       "      <td>9.64465</td>\n",
       "      <td>-6.49599</td>\n",
       "      <td>10.6034</td>\n",
       "      <td>-9.64661</td>\n",
       "      <td>1.90916</td>\n",
       "      <td>-0.186767</td>\n",
       "      <td>-4.03675</td>\n",
       "      <td>3.21464</td>\n",
       "      <td>-5.445380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7.07797</td>\n",
       "      <td>-7.26548</td>\n",
       "      <td>9.474140</td>\n",
       "      <td>9.06840</td>\n",
       "      <td>-8.41768</td>\n",
       "      <td>12.1046</td>\n",
       "      <td>-13.27290</td>\n",
       "      <td>3.46233</td>\n",
       "      <td>-1.225250</td>\n",
       "      <td>-5.04044</td>\n",
       "      <td>2.29165</td>\n",
       "      <td>-5.847240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X..language        X1       X2        X3        X4       X5       X6  \\\n",
       "0            1  10.22470 -5.84695  1.579330  11.07320 -6.74006  13.6003   \n",
       "1            0   4.89987 -4.39598  0.450237   6.70707 -6.72603  10.3793   \n",
       "2            0   2.91494 -4.19126  3.497830  10.37480 -6.31339  10.0329   \n",
       "3            0   4.88026 -6.57269  2.147220   9.64465 -6.49599  10.6034   \n",
       "4            1   7.07797 -7.26548  9.474140   9.06840 -8.41768  12.1046   \n",
       "\n",
       "         X7       X8        X9      X10      X11       X12  \n",
       "0  -8.34855  4.25603 -1.076250 -7.46205  4.02494 -0.636634  \n",
       "1  -9.64491  4.25752 -0.784425 -5.47746  2.21494 -7.412240  \n",
       "2 -10.61860  4.82322  1.174930 -3.97906  1.87674 -4.466670  \n",
       "3  -9.64661  1.90916 -0.186767 -4.03675  3.21464 -5.445380  \n",
       "4 -13.27290  3.46233 -1.225250 -5.04044  2.29165 -5.847240  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(47)\n",
    "\n",
    "train1 = pd.read_csv(\"train1-02.csv\")\n",
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X = train1.drop([\"X..language\"],1)\n",
    "y = np.c_[train1[\"X..language\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "np.random.seed(47)\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\")\n",
    "model1 = log_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "print(accuracy_score(y, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  0],\n",
       "       [ 0, 20]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X..X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.557100</td>\n",
       "      <td>-7.303100</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>13.65330</td>\n",
       "      <td>-4.46331</td>\n",
       "      <td>12.46410</td>\n",
       "      <td>-9.81588</td>\n",
       "      <td>1.87356</td>\n",
       "      <td>-1.345830</td>\n",
       "      <td>-8.63407</td>\n",
       "      <td>5.302520</td>\n",
       "      <td>-5.30560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.520500</td>\n",
       "      <td>-3.951550</td>\n",
       "      <td>0.195947</td>\n",
       "      <td>9.19618</td>\n",
       "      <td>-6.31148</td>\n",
       "      <td>9.14480</td>\n",
       "      <td>-8.80409</td>\n",
       "      <td>4.87974</td>\n",
       "      <td>-1.010500</td>\n",
       "      <td>-2.97694</td>\n",
       "      <td>3.927430</td>\n",
       "      <td>-7.16815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.595750</td>\n",
       "      <td>-2.466700</td>\n",
       "      <td>0.420945</td>\n",
       "      <td>5.33781</td>\n",
       "      <td>-5.52346</td>\n",
       "      <td>9.90535</td>\n",
       "      <td>-10.12440</td>\n",
       "      <td>4.74850</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>-1.66329</td>\n",
       "      <td>-0.500254</td>\n",
       "      <td>-5.02942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.213080</td>\n",
       "      <td>-8.050840</td>\n",
       "      <td>3.173940</td>\n",
       "      <td>8.01995</td>\n",
       "      <td>-7.03389</td>\n",
       "      <td>10.34740</td>\n",
       "      <td>-11.54210</td>\n",
       "      <td>3.36007</td>\n",
       "      <td>1.265040</td>\n",
       "      <td>-2.32166</td>\n",
       "      <td>0.821799</td>\n",
       "      <td>-6.95201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.458483</td>\n",
       "      <td>-0.811747</td>\n",
       "      <td>-3.077300</td>\n",
       "      <td>5.90479</td>\n",
       "      <td>-6.85130</td>\n",
       "      <td>8.41783</td>\n",
       "      <td>-8.78810</td>\n",
       "      <td>4.62265</td>\n",
       "      <td>-0.977714</td>\n",
       "      <td>-2.06965</td>\n",
       "      <td>2.230090</td>\n",
       "      <td>-5.59135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       X..X1        X2        X3        X4       X5        X6        X7  \\\n",
       "0  10.557100 -7.303100  0.008080  13.65330 -4.46331  12.46410  -9.81588   \n",
       "1   4.520500 -3.951550  0.195947   9.19618 -6.31148   9.14480  -8.80409   \n",
       "2  -2.595750 -2.466700  0.420945   5.33781 -5.52346   9.90535 -10.12440   \n",
       "3   5.213080 -8.050840  3.173940   8.01995 -7.03389  10.34740 -11.54210   \n",
       "4  -0.458483 -0.811747 -3.077300   5.90479 -6.85130   8.41783  -8.78810   \n",
       "\n",
       "        X8        X9      X10       X11      X12  \n",
       "0  1.87356 -1.345830 -8.63407  5.302520 -5.30560  \n",
       "1  4.87974 -1.010500 -2.97694  3.927430 -7.16815  \n",
       "2  4.74850 -0.090000 -1.66329 -0.500254 -5.02942  \n",
       "3  3.36007  1.265040 -2.32166  0.821799 -6.95201  \n",
       "4  4.62265 -0.977714 -2.06965  2.230090 -5.59135  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing data\n",
    "test1 = pd.read_csv(\"test1-02.csv\")\n",
    "labels = pd.read_csv(\"labels1-02.csv\")\n",
    "test1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "np.random.seed(45)\n",
    "y_pred_test = model1.predict(test1)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Testing accuracy\n",
    "print(accuracy_score(labels, y_pred_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 0],\n",
       "       [0, 9]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred_1,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set “train1-02.csv” was used to create a logistic regression model in order to classify examples from speakers for two different languages. The data set consists of 40 observations of speakers, and 13 variables. There are 12 predictor variables labelled from “X1” to “X12”, and a categorical target variable “X..language” with values 0 and 1 representing  one of the two different languages respectively. \n",
    "\n",
    "The target variable “X..language” was removed from the data frame so that only the 12 predictor variables remained to be assigned to a new variable “X”. The “X..language” variable was assigned to variable “y”. The sklearn classification algorithm LogisticRegression was used to fit a logistic regression model to the training data. The proportion of correct predictions, or the training accuracy for the model, was 100%. A confusion matrix showed that there were zero misclassifications, with classifications distributed evenly between the two classes (20 observations in each class). \n",
    "\n",
    "The test data set was the applied to the trained model, and the predicted values were computed. A comparison with the true test outcomes revealed the test accuracy to also be 100%, with no misclassifications and observations spread evenly between classes.\n",
    "The code was checked to verify that such a high test accuracy was not the result of \n",
    "data leakage, or accidently using data from the training set when testing the model, leading other causes for this result to be considered. From examining the training and test sets, it can be seen that both are very similar, which may have led the testing data to perform at a similar level to the training data. \n",
    "\n",
    "Although the training and test accuracy are extremely high, this does not necessarily mean that the model may be considered good. As previously mentioned, the test accuracy result may be due similarity between training testing data, or the small size of the testing data set. As the training accuracy was shown to be 100%, there is a possibility that the model has overfit. As such, it cannot be guaranteed the model would generalise well to different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X..language</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>7.56899</td>\n",
       "      <td>-3.341920</td>\n",
       "      <td>7.095370</td>\n",
       "      <td>14.11000</td>\n",
       "      <td>-3.25970</td>\n",
       "      <td>13.4015</td>\n",
       "      <td>-5.29880</td>\n",
       "      <td>7.44277</td>\n",
       "      <td>2.96299</td>\n",
       "      <td>0.265848</td>\n",
       "      <td>4.14782</td>\n",
       "      <td>-5.72775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>7.66209</td>\n",
       "      <td>-0.809953</td>\n",
       "      <td>3.337540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.16989</td>\n",
       "      <td>12.2864</td>\n",
       "      <td>-5.66250</td>\n",
       "      <td>8.02133</td>\n",
       "      <td>2.13109</td>\n",
       "      <td>0.164655</td>\n",
       "      <td>7.06902</td>\n",
       "      <td>-4.02656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>5.20720</td>\n",
       "      <td>0.316089</td>\n",
       "      <td>2.299860</td>\n",
       "      <td>11.19260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.56881</td>\n",
       "      <td>10.26090</td>\n",
       "      <td>2.60361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.39893</td>\n",
       "      <td>2.35551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2.68311</td>\n",
       "      <td>2.329850</td>\n",
       "      <td>0.064298</td>\n",
       "      <td>9.04638</td>\n",
       "      <td>-3.70971</td>\n",
       "      <td>11.5594</td>\n",
       "      <td>-5.64650</td>\n",
       "      <td>7.76425</td>\n",
       "      <td>2.16388</td>\n",
       "      <td>1.071940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.44976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.54584</td>\n",
       "      <td>0.674888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.47940</td>\n",
       "      <td>-2.38186</td>\n",
       "      <td>13.0469</td>\n",
       "      <td>-6.98280</td>\n",
       "      <td>7.89009</td>\n",
       "      <td>3.05159</td>\n",
       "      <td>1.478300</td>\n",
       "      <td>2.64134</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X..language       X1        X2        X3        X4       X5       X6  \\\n",
       "0            6  7.56899 -3.341920  7.095370  14.11000 -3.25970  13.4015   \n",
       "1            6  7.66209 -0.809953  3.337540       NaN -3.16989  12.2864   \n",
       "2            6  5.20720  0.316089  2.299860  11.19260      NaN      NaN   \n",
       "3            6  2.68311  2.329850  0.064298   9.04638 -3.70971  11.5594   \n",
       "4            6  0.54584  0.674888       NaN   8.47940 -2.38186  13.0469   \n",
       "\n",
       "        X7        X8       X9       X10      X11      X12  \n",
       "0 -5.29880   7.44277  2.96299  0.265848  4.14782 -5.72775  \n",
       "1 -5.66250   8.02133  2.13109  0.164655  7.06902 -4.02656  \n",
       "2 -6.56881  10.26090  2.60361       NaN  7.39893  2.35551  \n",
       "3 -5.64650   7.76425  2.16388  1.071940      NaN -2.44976  \n",
       "4 -6.98280   7.89009  3.05159  1.478300  2.64134      NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "train2 = pd.read_csv(\"train2-01.csv\")\n",
    "train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing values\n",
    "train2.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.56899</td>\n",
       "      <td>-3.341920</td>\n",
       "      <td>7.095370</td>\n",
       "      <td>14.11000</td>\n",
       "      <td>-3.259700</td>\n",
       "      <td>13.40150</td>\n",
       "      <td>-5.29880</td>\n",
       "      <td>7.44277</td>\n",
       "      <td>2.96299</td>\n",
       "      <td>0.265848</td>\n",
       "      <td>4.147820</td>\n",
       "      <td>-5.727750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>7.66209</td>\n",
       "      <td>-0.809953</td>\n",
       "      <td>3.337540</td>\n",
       "      <td>10.49096</td>\n",
       "      <td>-3.169890</td>\n",
       "      <td>12.28640</td>\n",
       "      <td>-5.66250</td>\n",
       "      <td>8.02133</td>\n",
       "      <td>2.13109</td>\n",
       "      <td>0.164655</td>\n",
       "      <td>7.069020</td>\n",
       "      <td>-4.026560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.20720</td>\n",
       "      <td>0.316089</td>\n",
       "      <td>2.299860</td>\n",
       "      <td>11.19260</td>\n",
       "      <td>-1.689422</td>\n",
       "      <td>11.97976</td>\n",
       "      <td>-6.56881</td>\n",
       "      <td>10.26090</td>\n",
       "      <td>2.60361</td>\n",
       "      <td>-0.853989</td>\n",
       "      <td>7.398930</td>\n",
       "      <td>2.355510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.68311</td>\n",
       "      <td>2.329850</td>\n",
       "      <td>0.064298</td>\n",
       "      <td>9.04638</td>\n",
       "      <td>-3.709710</td>\n",
       "      <td>11.55940</td>\n",
       "      <td>-5.64650</td>\n",
       "      <td>7.76425</td>\n",
       "      <td>2.16388</td>\n",
       "      <td>1.071940</td>\n",
       "      <td>4.893598</td>\n",
       "      <td>-2.449760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.54584</td>\n",
       "      <td>0.674888</td>\n",
       "      <td>3.856960</td>\n",
       "      <td>8.47940</td>\n",
       "      <td>-2.381860</td>\n",
       "      <td>13.04690</td>\n",
       "      <td>-6.98280</td>\n",
       "      <td>7.89009</td>\n",
       "      <td>3.05159</td>\n",
       "      <td>1.478300</td>\n",
       "      <td>2.641340</td>\n",
       "      <td>-1.941457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0        1         2         3         4         5         6        7   \\\n",
       "0  6.0  7.56899 -3.341920  7.095370  14.11000 -3.259700  13.40150 -5.29880   \n",
       "1  6.0  7.66209 -0.809953  3.337540  10.49096 -3.169890  12.28640 -5.66250   \n",
       "2  6.0  5.20720  0.316089  2.299860  11.19260 -1.689422  11.97976 -6.56881   \n",
       "3  6.0  2.68311  2.329850  0.064298   9.04638 -3.709710  11.55940 -5.64650   \n",
       "4  6.0  0.54584  0.674888  3.856960   8.47940 -2.381860  13.04690 -6.98280   \n",
       "\n",
       "         8        9         10        11        12  \n",
       "0   7.44277  2.96299  0.265848  4.147820 -5.727750  \n",
       "1   8.02133  2.13109  0.164655  7.069020 -4.026560  \n",
       "2  10.26090  2.60361 -0.853989  7.398930  2.355510  \n",
       "3   7.76425  2.16388  1.071940  4.893598 -2.449760  \n",
       "4   7.89009  3.05159  1.478300  2.641340 -1.941457  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(47)\n",
    "#replace missing values with k nearest neighbours imputation\n",
    "#takes average of k nearest neighbours to missing values\n",
    "from sklearn.impute import KNNImputer \n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "train2_impute = imputer.fit_transform(train2)\n",
    "train2_impute = pd.DataFrame(train2_impute) #dataframe\n",
    "train2_impute.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 7.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = train2_impute.drop([0],1)\n",
    "y2 = np.c_[train2_impute[0]]\n",
    "np.unique(y2) #class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "np.random.seed(47)\n",
    "model2 = log_reg.fit(X2, y2.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# Training accuracy\n",
    "y_pred_train2 = model2.predict(X2) \n",
    "print(accuracy_score(y2, y_pred_train2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  0],\n",
       "       [ 0, 20]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred_train2,y2) #training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X..X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.27083</td>\n",
       "      <td>3.543050</td>\n",
       "      <td>12.43500</td>\n",
       "      <td>-3.073420</td>\n",
       "      <td>9.8186</td>\n",
       "      <td>-6.33397</td>\n",
       "      <td>10.93810</td>\n",
       "      <td>1.058550</td>\n",
       "      <td>-7.425450</td>\n",
       "      <td>8.12394</td>\n",
       "      <td>1.579180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.63742</td>\n",
       "      <td>1.26599</td>\n",
       "      <td>5.124260</td>\n",
       "      <td>9.79453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.5637</td>\n",
       "      <td>-6.11971</td>\n",
       "      <td>8.27859</td>\n",
       "      <td>3.032970</td>\n",
       "      <td>2.018400</td>\n",
       "      <td>6.50936</td>\n",
       "      <td>1.636920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.12113</td>\n",
       "      <td>4.25289</td>\n",
       "      <td>1.034530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.236166</td>\n",
       "      <td>13.8259</td>\n",
       "      <td>-7.34798</td>\n",
       "      <td>8.32609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.437000</td>\n",
       "      <td>6.70727</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.74538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.525200</td>\n",
       "      <td>5.86719</td>\n",
       "      <td>-5.589730</td>\n",
       "      <td>15.3742</td>\n",
       "      <td>-5.45422</td>\n",
       "      <td>7.15645</td>\n",
       "      <td>-0.366833</td>\n",
       "      <td>-0.908566</td>\n",
       "      <td>8.50961</td>\n",
       "      <td>-0.574648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.92624</td>\n",
       "      <td>3.02930</td>\n",
       "      <td>0.316425</td>\n",
       "      <td>10.46540</td>\n",
       "      <td>2.975700</td>\n",
       "      <td>11.1443</td>\n",
       "      <td>-7.69285</td>\n",
       "      <td>9.63877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.554560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X..X1       X2        X3        X4        X5       X6       X7        X8  \\\n",
       "0      NaN  0.27083  3.543050  12.43500 -3.073420   9.8186 -6.33397  10.93810   \n",
       "1  5.63742  1.26599  5.124260   9.79453       NaN  15.5637 -6.11971   8.27859   \n",
       "2  1.12113  4.25289  1.034530       NaN -0.236166  13.8259 -7.34798   8.32609   \n",
       "3  8.74538      NaN  9.525200   5.86719 -5.589730  15.3742 -5.45422   7.15645   \n",
       "4 -2.92624  3.02930  0.316425  10.46540  2.975700  11.1443 -7.69285   9.63877   \n",
       "\n",
       "         X9       X10      X11       X12  \n",
       "0  1.058550 -7.425450  8.12394  1.579180  \n",
       "1  3.032970  2.018400  6.50936  1.636920  \n",
       "2       NaN  1.437000  6.70727       NaN  \n",
       "3 -0.366833 -0.908566  8.50961 -0.574648  \n",
       "4       NaN -2.554560      NaN  0.370682  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing data\n",
    "test2 = pd.read_csv(\"test2-01.csv\")\n",
    "test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.568688</td>\n",
       "      <td>0.270830</td>\n",
       "      <td>3.543050</td>\n",
       "      <td>12.43500</td>\n",
       "      <td>-3.073420</td>\n",
       "      <td>9.8186</td>\n",
       "      <td>-6.33397</td>\n",
       "      <td>10.93810</td>\n",
       "      <td>1.058550</td>\n",
       "      <td>-7.425450</td>\n",
       "      <td>8.123940</td>\n",
       "      <td>1.579180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.637420</td>\n",
       "      <td>1.265990</td>\n",
       "      <td>5.124260</td>\n",
       "      <td>9.79453</td>\n",
       "      <td>-3.538656</td>\n",
       "      <td>15.5637</td>\n",
       "      <td>-6.11971</td>\n",
       "      <td>8.27859</td>\n",
       "      <td>3.032970</td>\n",
       "      <td>2.018400</td>\n",
       "      <td>6.509360</td>\n",
       "      <td>1.636920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.121130</td>\n",
       "      <td>4.252890</td>\n",
       "      <td>1.034530</td>\n",
       "      <td>10.12011</td>\n",
       "      <td>-0.236166</td>\n",
       "      <td>13.8259</td>\n",
       "      <td>-7.34798</td>\n",
       "      <td>8.32609</td>\n",
       "      <td>2.350500</td>\n",
       "      <td>1.437000</td>\n",
       "      <td>6.707270</td>\n",
       "      <td>0.205466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.745380</td>\n",
       "      <td>0.301451</td>\n",
       "      <td>9.525200</td>\n",
       "      <td>5.86719</td>\n",
       "      <td>-5.589730</td>\n",
       "      <td>15.3742</td>\n",
       "      <td>-5.45422</td>\n",
       "      <td>7.15645</td>\n",
       "      <td>-0.366833</td>\n",
       "      <td>-0.908566</td>\n",
       "      <td>8.509610</td>\n",
       "      <td>-0.574648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.926240</td>\n",
       "      <td>3.029300</td>\n",
       "      <td>0.316425</td>\n",
       "      <td>10.46540</td>\n",
       "      <td>2.975700</td>\n",
       "      <td>11.1443</td>\n",
       "      <td>-7.69285</td>\n",
       "      <td>9.63877</td>\n",
       "      <td>0.991773</td>\n",
       "      <td>-2.554560</td>\n",
       "      <td>6.866488</td>\n",
       "      <td>0.370682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4        5        6   \\\n",
       "0  6.568688  0.270830  3.543050  12.43500 -3.073420   9.8186 -6.33397   \n",
       "1  5.637420  1.265990  5.124260   9.79453 -3.538656  15.5637 -6.11971   \n",
       "2  1.121130  4.252890  1.034530  10.12011 -0.236166  13.8259 -7.34798   \n",
       "3  8.745380  0.301451  9.525200   5.86719 -5.589730  15.3742 -5.45422   \n",
       "4 -2.926240  3.029300  0.316425  10.46540  2.975700  11.1443 -7.69285   \n",
       "\n",
       "         7         8         9         10        11  \n",
       "0  10.93810  1.058550 -7.425450  8.123940  1.579180  \n",
       "1   8.27859  3.032970  2.018400  6.509360  1.636920  \n",
       "2   8.32609  2.350500  1.437000  6.707270  0.205466  \n",
       "3   7.15645 -0.366833 -0.908566  8.509610 -0.574648  \n",
       "4   9.63877  0.991773 -2.554560  6.866488  0.370682  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#impute missing values in test set\n",
    "test2_impute = imputer.fit_transform(test2)\n",
    "test2_impute = pd.DataFrame(test2_impute) #dataframe\n",
    "test2_impute.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 6., 6., 7., 6., 6., 6., 7., 6., 7., 6., 7., 6., 6., 7., 7., 6.,\n",
       "       6.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict\n",
    "np.random.seed(47)\n",
    "y_pred_2 = model2.predict(test2_impute)\n",
    "y_pred_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set “train2-01.csv” was used to create a logistic regression model to classify examples from speakers as belonging to one of two different language classes, similarly to question 1. The training data set consists of 12 predictor variables and one categorical target variable containing two values 6 and 7 as the labels for the two language classes. \n",
    "\n",
    "The training data set also contains 42 missing values, meaning that the data required pre-processing before a model could be built. The method of dealing with missing data by removing such instances was considered, however this approach would be detrimental here as the data set is very small, containing only 40 observations and 12 columns. Such an approach may lead to a significant loss of important information in the data. Imputation as a method of dealing with missing data was selected, whereby information from predictors in the training set is used to estimate the values of missing data. Replacing the missing values with the average of their corresponding columns was also considered, however such a method of imputation may be inaccurate as outliers can significantly influence the computed values. \n",
    "\n",
    "Thus, a method of imputation using a K- nearest neighbours approach was chosen. With this method, a missing value is replaced by taking the average of its  k- nearest samples. This has the advantage of keeping imputed missing values within appropriate range of the values in the training data. The KNNImputer class from sklearn was used to carry out this method of missing value imputation and to create a new training set upon which the regression model could be built on.\n",
    "\n",
    "The sklearn classification algorithm LogisticRegression was used to fit a logistic regression model to the new training set with replaced missing values. The resulting proportion of correct class predictions was 100%, with no misclassifications and speaker example observations distributed evenly between the two language classes. \n",
    "The test data set also contained missing values, and the same K- nearest neighbours approach was used for imputation. This new test set was then applied to the logistic regression model and the predicted test outputs were computed. \n",
    "\n",
    "As the training accuracy of the model was 100%, this may indicate that overfitting has occurred. As such, the performance of the model on the test data cannot be guaranteed to perform well as the model may not generalise well to unseen data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X..language</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9.9178</td>\n",
       "      <td>5.08356</td>\n",
       "      <td>18.3282</td>\n",
       "      <td>13.6834</td>\n",
       "      <td>7.40002</td>\n",
       "      <td>27.6248</td>\n",
       "      <td>1.829200</td>\n",
       "      <td>24.5021</td>\n",
       "      <td>15.9707</td>\n",
       "      <td>-0.048114</td>\n",
       "      <td>17.7645</td>\n",
       "      <td>-1.623060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>16.2788</td>\n",
       "      <td>4.45568</td>\n",
       "      <td>16.0435</td>\n",
       "      <td>19.9961</td>\n",
       "      <td>7.67195</td>\n",
       "      <td>26.0439</td>\n",
       "      <td>4.007120</td>\n",
       "      <td>26.4307</td>\n",
       "      <td>18.8938</td>\n",
       "      <td>-6.861160</td>\n",
       "      <td>17.1780</td>\n",
       "      <td>0.309046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>17.9197</td>\n",
       "      <td>-1.49624</td>\n",
       "      <td>19.2557</td>\n",
       "      <td>19.2635</td>\n",
       "      <td>3.25510</td>\n",
       "      <td>24.1119</td>\n",
       "      <td>6.860250</td>\n",
       "      <td>21.5101</td>\n",
       "      <td>17.3434</td>\n",
       "      <td>-1.831600</td>\n",
       "      <td>19.7689</td>\n",
       "      <td>4.626680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>16.7873</td>\n",
       "      <td>2.47533</td>\n",
       "      <td>24.7522</td>\n",
       "      <td>20.0013</td>\n",
       "      <td>5.18935</td>\n",
       "      <td>27.8916</td>\n",
       "      <td>0.734509</td>\n",
       "      <td>24.9686</td>\n",
       "      <td>18.9031</td>\n",
       "      <td>-2.969510</td>\n",
       "      <td>15.7605</td>\n",
       "      <td>-1.101710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10.7314</td>\n",
       "      <td>7.43060</td>\n",
       "      <td>18.2488</td>\n",
       "      <td>15.4827</td>\n",
       "      <td>7.63276</td>\n",
       "      <td>25.2403</td>\n",
       "      <td>0.167220</td>\n",
       "      <td>24.2909</td>\n",
       "      <td>18.7210</td>\n",
       "      <td>-0.321854</td>\n",
       "      <td>16.5543</td>\n",
       "      <td>-4.162160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X..language       X1       X2       X3       X4       X5       X6  \\\n",
       "0            1   9.9178  5.08356  18.3282  13.6834  7.40002  27.6248   \n",
       "1            2  16.2788  4.45568  16.0435  19.9961  7.67195  26.0439   \n",
       "2            5  17.9197 -1.49624  19.2557  19.2635  3.25510  24.1119   \n",
       "3            2  16.7873  2.47533  24.7522  20.0013  5.18935  27.8916   \n",
       "4            1  10.7314  7.43060  18.2488  15.4827  7.63276  25.2403   \n",
       "\n",
       "         X7       X8       X9       X10      X11       X12  \n",
       "0  1.829200  24.5021  15.9707 -0.048114  17.7645 -1.623060  \n",
       "1  4.007120  26.4307  18.8938 -6.861160  17.1780  0.309046  \n",
       "2  6.860250  21.5101  17.3434 -1.831600  19.7689  4.626680  \n",
       "3  0.734509  24.9686  18.9031 -2.969510  15.7605 -1.101710  \n",
       "4  0.167220  24.2909  18.7210 -0.321854  16.5543 -4.162160  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "train3 = pd.read_csv(\"train3-125.csv\")\n",
    "train3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3 = train3.drop([\"X..language\"],1)\n",
    "y3 = np.c_[train3[\"X..language\"]]\n",
    "np.unique(y3) # classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X..X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.54980</td>\n",
       "      <td>3.62534</td>\n",
       "      <td>27.8057</td>\n",
       "      <td>9.10427</td>\n",
       "      <td>20.12290</td>\n",
       "      <td>18.1854</td>\n",
       "      <td>8.94644</td>\n",
       "      <td>21.9994</td>\n",
       "      <td>10.0720</td>\n",
       "      <td>13.466200</td>\n",
       "      <td>4.97376</td>\n",
       "      <td>6.16664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.74460</td>\n",
       "      <td>-1.93853</td>\n",
       "      <td>19.0690</td>\n",
       "      <td>23.79500</td>\n",
       "      <td>7.99254</td>\n",
       "      <td>30.3174</td>\n",
       "      <td>4.34176</td>\n",
       "      <td>18.3023</td>\n",
       "      <td>19.6996</td>\n",
       "      <td>-7.990290</td>\n",
       "      <td>19.38110</td>\n",
       "      <td>5.12450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.83680</td>\n",
       "      <td>2.67344</td>\n",
       "      <td>23.4723</td>\n",
       "      <td>18.84740</td>\n",
       "      <td>7.22676</td>\n",
       "      <td>28.6772</td>\n",
       "      <td>1.68494</td>\n",
       "      <td>22.4163</td>\n",
       "      <td>18.1935</td>\n",
       "      <td>-0.644812</td>\n",
       "      <td>16.15120</td>\n",
       "      <td>-1.81631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.94236</td>\n",
       "      <td>7.48171</td>\n",
       "      <td>14.4068</td>\n",
       "      <td>14.22270</td>\n",
       "      <td>8.51825</td>\n",
       "      <td>26.4827</td>\n",
       "      <td>3.30106</td>\n",
       "      <td>23.5244</td>\n",
       "      <td>19.2834</td>\n",
       "      <td>0.932259</td>\n",
       "      <td>16.40540</td>\n",
       "      <td>1.71311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.26000</td>\n",
       "      <td>3.19063</td>\n",
       "      <td>19.7457</td>\n",
       "      <td>14.55730</td>\n",
       "      <td>8.14645</td>\n",
       "      <td>29.6702</td>\n",
       "      <td>3.13715</td>\n",
       "      <td>20.7950</td>\n",
       "      <td>19.5352</td>\n",
       "      <td>1.852130</td>\n",
       "      <td>19.15230</td>\n",
       "      <td>3.14633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      X..X1       X2       X3        X4        X5       X6       X7       X8  \\\n",
       "0  15.54980  3.62534  27.8057   9.10427  20.12290  18.1854  8.94644  21.9994   \n",
       "1  21.74460 -1.93853  19.0690  23.79500   7.99254  30.3174  4.34176  18.3023   \n",
       "2  13.83680  2.67344  23.4723  18.84740   7.22676  28.6772  1.68494  22.4163   \n",
       "3   8.94236  7.48171  14.4068  14.22270   8.51825  26.4827  3.30106  23.5244   \n",
       "4  13.26000  3.19063  19.7457  14.55730   8.14645  29.6702  3.13715  20.7950   \n",
       "\n",
       "        X9        X10       X11      X12  \n",
       "0  10.0720  13.466200   4.97376  6.16664  \n",
       "1  19.6996  -7.990290  19.38110  5.12450  \n",
       "2  18.1935  -0.644812  16.15120 -1.81631  \n",
       "3  19.2834   0.932259  16.40540  1.71311  \n",
       "4  19.5352   1.852130  19.15230  3.14633  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing data\n",
    "test3 = pd.read_csv(\"test3-125.csv\")\n",
    "test3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X3_scaled = scaler.fit_transform(X3)\n",
    "test3_scaled = scaler.fit_transform(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01, accuracy = 98.52941176470588\n",
      "C = 0.1, accuracy = 98.52941176470588\n",
      "C = 1, accuracy = 98.52941176470588\n",
      "C = 10, accuracy = 98.52941176470588\n",
      "C = 100, accuracy = 98.52941176470588\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Margins = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "resAccuracy = []\n",
    "for m in Margins:\n",
    "    svm_clf_cv = SVC(kernel=\"rbf\", gamma='scale', C=1, max_iter=1e4)   # using iter=1e4\n",
    "    svm_clf_cv.fit(X3_scaled,y3.ravel())\n",
    "    y_pred = svm_clf_cv.predict(X3_scaled)\n",
    "    accCurr = accuracy_score(y3.ravel(), y_pred)*100\n",
    "    print(f'C = {m}, accuracy = {accCurr}')\n",
    "    resAccuracy.append(accCurr)\n",
    "    \n",
    "plt.figure(figsize=(6,4))\n",
    "plt.semilogx(Margins, resAccuracy, '*-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.52941176470588"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "np.random.seed(52)\n",
    "svm_clf3 = SVC(kernel=\"rbf\", gamma='scale', C=1, max_iter=1e4)\n",
    "model3_scaled = svm_clf3.fit(X3_scaled,y3.ravel())\n",
    "#training error\n",
    "y3_pred_scaled = model3_scaled.predict(X3_scaled)\n",
    "accuracy_score(y3, y3_pred_scaled) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  0,  0],\n",
       "       [ 0, 20,  1],\n",
       "       [ 0,  0, 27]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y3_pred_scaled, y3) #1 missclassfication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 5, 2, 1, 5, 5, 2, 5, 1, 5, 2, 1, 1, 5, 2, 5, 5, 2, 1, 5, 5,\n",
       "       5, 5, 2, 1, 5, 2, 5, 5, 1, 1, 5, 2, 5, 1, 1, 5, 2, 5, 2, 1, 5, 2,\n",
       "       1, 2, 5, 1, 1, 2, 5, 2, 5, 2, 5, 5, 1, 2, 1, 1, 1, 2, 5, 2, 5, 1,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test predictions\n",
    "y3_test_pred = svm_clf3.predict(test3_scaled)\n",
    "y3_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 20, 2: 21, 5: 27}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y3_test_pred, return_counts=True)\n",
    "dict(zip(unique, counts)) #how many in each predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.9428571428571428 (pvalue : 0.009900990099009901)\n"
     ]
    }
   ],
   "source": [
    "# Cross- validation and permutation\n",
    "np.random.seed(53)\n",
    "svm_clf = SVC(kernel=\"rbf\", gamma='scale', C=1, max_iter=1e4)\n",
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    svm_clf, X3_scaled, y3.ravel(), scoring=\"accuracy\", cv=10, n_permutations=100, n_jobs=1)\n",
    "print(\"Test accuracy %s (pvalue : %s)\" % (score, pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set “train-3.csv” was used to create a support vector classification model in order to classify speaker examples to one of three different language classes.  The data set consists of 12 predictor variables and one categorical target values with values 1,2 and 5 for each of the classes representing a different language.\n",
    "\n",
    "The data was scaled using StandardScaler from sklearn, so that the data would have a mean of 0 and standard deviation of 1. Cross-validation was used to determine the best value for the regularisation parameter C, in order to determine a hyperplane that separates the training data into classes with greater accuracy. From the resulting plot, it could be determined that the accuracy remains the same regardless of the margin and the value of C. \n",
    "\n",
    "The scaled data was then fit to a support vector machine classifier model with the regularisation parameter set to 1, and the proportion of correct class predictions or training accuracy score was determined to be 98.53%. A confusion matrix showed there to be one misclassification, with the rest of the observations being spread generally evenly among the three language classes, with 20 observations in the first two classes and 27 in the third class. The test data was also scaled and subsequently applied to the support vector classification model to predict the class labels. The predicted class labels for test data were spread relatively evenly across the three language classes, with the distribution being similar to that of the confusion matrix showing the correct classifications for the training data. \n",
    "\n",
    "In order to validate that such a model would perform well on test data, 10-fold cross validation was used due to the small sample size, splitting the training data into training and test sets. The significance of the cross-validated test accuracy score was evaluated by permutation, randomising the labels and repeating the classification method 100 times. A p-value is determined which provides an approximation of the probability that the accuracy score would be obtained by chance. This is calculated by determining the percentage of permutations for which the score is greater than the true classification score. In order to carry out this permutation significance testing and cross- validation, the permutation_test_score from sklearn was used. The regularisation parameter C was set to 1 as previously, and the same kernel was used. The test accuracy was determined to be 94.29% and the p-value to be 0.009, suggesting that the model is significant at the 0.01 level. From this it may be inferred that there is a significant relationship between the speaker example predictor variables and the language classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Neighbours Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train4 = pd.read_csv(\"train3-125.csv\") #same data as Q3\n",
    "X4 = train4.drop([\"X..language\"],1)\n",
    "y4 = np.c_[train4[\"X..language\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing data\n",
    "test4 = pd.read_csv(\"test3-125.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "k = 3\n",
      "k = 5\n",
      "k = 7\n",
      "k = 9\n",
      "k = 11\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD7CAYAAABkO19ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAdGklEQVR4nO3deXRV9b338fc3cwIJYQgyDwkylEGoERyA2FoHyq2K9lKqAkUUaJ/eZ91bh/Y+tdZW2nV7u+5tayeBMqhoW6312trWtg4FRQWCA4qAzAgCCRICSSAJyff54xy6cmlogTPsM3xea7Egex/P/u5l+GTzO/t8jrk7IiKSujKCHkBERGJLQS8ikuIU9CIiKU5BLyKS4hT0IiIpLivoAdrTrVs3HzBgQNBjiIgklXXr1h1095JTtydk0A8YMIDKysqgxxARSSpmtqu97Vq6ERFJcQp6EZEUp6AXEUlxCnoRkRSnoBcRSXFnFPRm9kUzqzSzRjNbdsq+K8xsk5k1mNmLZta/zb5cM1tiZkfMbL+ZfSnK8/8vVUeOM3XBq1QdPR7Lw4iIJJUzvaL/AJgPLGm70cy6Ab8GvgZ0ASqBX7Z5yH3A+UB/4GPA3WZ2TWQjn94Dz29h7c5DPPDcllgdQkQk6ZzRffTu/msAMysH+rTZdQOwwd2fCO+/DzhoZkPdfRMwA5jl7jVAjZktAj4HPBu1MwCG3PMHGk+0/vXr5at3s3z1bnKzMtg8f1I0DyUiknQiXaMfDrx18gt3rwe2AcPNrDPQq+3+8J+Ht/dEZjYnvDxUWV1dfVZDvHT3x7h2dC8yLPR1XnYG143uxUtf/thZPY+ISCqKNOg7ArWnbKsFCsP7OGX/yX1/w90Xunu5u5eXlPzNO3j/ru5FeRTmZnHyM1Qam1spzM2ie2HeWT2PiEgqijTo64CiU7YVAUfD+zhl/8l9UXewrpGbL+7PyN5F5GVncuBoYywOIyKSdCIN+g3ABSe/MLMOQBmhdfsaYF/b/eE/b4jwmO1aML2c+deP4I6rhnCsuYWrh/eIxWFERJLOmd5emWVmeUAmkGlmeWaWBTwFjDCzG8P77wXWh1+IBXgYuMfMOpvZUOB2YFnUz6KNisElDO1RyMKV22ht1efhioic6RX9PcAx4CvALeE/3+Pu1cCNwLeAGmAcMK3Nf/d1Qi/O7gJWAN9196jecXMqM2NuRSnvHajjxc1VsTyUiEhSMPfEu+otLy/3SGqKm1taufy7f6F3cT6Pz7skipOJiCQuM1vn7uWnbk/JCoTszAxmjx/Imp2HWLerJuhxREQClZJBD/CZi/rSKT+bhSu3BT2KiEigUjboO+RmMeOS/vzp3QNsq677x/+BiEiKStmgB5h56QByMjNYtHJ70KOIiAQmpYO+W8dc/rm8D79+fS9VR9RoKSLpKaWDHuC28aWcaG1l6Ss7gx5FRCQQKR/0A7p1YNKInix/bRdHjzcHPY6ISNylfNADzK0o5ejxE/x8ze6gRxERibu0CPpRfYq5tKwri1/eQVOb3noRkXSQFkEPMLeijANHGnn6zb1BjyIiEldpE/QTz+/G0B6FLFi5XWVnIpJW0ibozYx5FWVsrarjhU0qOxOR9JE2QQ8weVRPehfns0C1CCKSRtIq6LMzM7htwkDW7qxh3a5DQY8jIhIXaRX0ECo7Ky7I5sEVqkUQkfSQdkFfkJPFjIv78+d3D7C1SmVnIpL60i7oIVR2lpulsjMRSQ9pGfRdO+YytbwvT72xlwMqOxORFJeWQQ9w24SBnGhtZcmqHUGPIiISU2kb9P27dmDSyJ489tpujqjsTERSWNoGPcC8iWUcbTzBz1er7ExEUldaB/3IPp24bFBXlqzaQeOJlqDHERGJibQOeoC5E8NlZ298EPQoIiIxkfZBP+H8bnykZxELVm5T2ZmIpKS0D3ozY25FKduq63leZWcikoLSPugBJo8Ml52tUNmZiKQeBT2QlZnB7RMGUrmrhsqdKjsTkdSioA+belFfOqvsTERSkII+rCAnixmXDOC5jQfYWnU06HFERKJGQd/GjEv6k5edwUKVnYlIClHQt9G27Gx/rcrORCQ1KOhPcdv4UlpanaUqOxORFKGgP0W/rgVMHtWLR1er7ExEUoOCvh1zJ5ZS13iCx1R2JiIpQEHfjhG9OzF+UDeWvKyyMxFJfhEHvZkNM7MXzKzWzLaa2ZQ2+24Lb6szs2fNrFekx4uXuRWlVB1t5H/e2Bv0KCIiEYko6M0sC3gaeAboAswBlpvZYDOrAL4NXBfetwP4eWTjxs/4Qd0Y3quIBSu3q+xMRJJapFf0Q4FewPfcvcXdXwBWAdOBTwFPuPsGd28C7gcmmllZhMeMi1DZWRnbq+t5buOBoMcRETlnkQa9nWbbiPDv1s5jR7T7RGZzzKzSzCqrq6sjHCs6PjmiB3065/Pgim2466peRJJTpEG/CagC7jKzbDO7CqgACoDfA1PNbJSZ5QP3Ah7e9zfcfaG7l7t7eUlJSYRjRUeo7KyU13cfpnJXTdDjiIick4iC3t2bgeuBycB+4A7gcWCPuz8PfB14EtgF7ASOAnsiOWa8TS0PlZ2pwlhEklXEd924+3p3r3D3ru5+NVAKrAnv+7G7n+/u3QkFfhbwTqTHjKf8nExmXjqA5zZWseWAys5EJPlE4/bKUWaWZ2YFZnYn0BNYFt42wkL6AQuBH7h70q2BzLhkAHnZGSxQ2ZmIJKFovGFqOrCP0Fr9FcCV7t4I5AGPAXWErvBfBb4WhePFXZcOOUy7qB9Pv7mXfbXHgh5HROSsRGPp5i537+zuHd19krtvDW8/7O6j3L2Du/dw939396R9m+ns8QNpdVi6amfQo4iInBVVIJyhvl0KmDyyJ4+t3k3tMZWdiUjyUNCfhTnhsrNHV+8KehQRkTOmoD8LI3p3YsL53Vi6aifHm5N2FUpE0oyC/izNqyijWmVnIpJEFPRn6dKyrozoXcRClZ2JSJJQ0J8lM2PuxDK2H6znT++q7ExEEp+C/hxMGtGDfl0KVHYmIklBQX8OQmVnA3nz/cOs3Zl0b/QVkTSjoD9Hn76wL1065KjsTEQSnoL+HOXnZDLzkgE8v6mKzftVdiYiiUtBH4EZl/QnPzuThSo7E5EEpqCPQOcOOXzmor4qOxORhKagj9Ds8QNxYMnLO4IeRUSkXQr6CPXtUsA/jQqXnTWo7ExEEo+CPgrmTiyjvqmF5So7E5EEpKCPgo/0KmLi4BKVnYlIQlLQR8m8iaUcrGvkKZWdiUiCUdBHySVlXRnZuxMLV26nRWVnIpJAFPRRYmbMqyhjx8F6/vzu/qDHERH5KwV9FF0TLjv76YrtKjsTkYShoI+izAzj9omlvPX+YdbsOBT0OCIigII+6v75wj507ZDDgyo7E5EEoaCPsrzsTD536QBe3FzNpv1Hgh5HRERBHwvTVXYmIglEQR8DxQU5TBvbl9+8+QEfHFbZmYgES0EfIyfLzhar7ExEAqagj5E+nQu49oJe/HyNys5EJFgK+hiaM7GUBpWdiUjAFPQxNKxnERWDS1i6aofKzkQkMAr6GJtbUcrBuiaefH1P0KOISJpS0MfYJaVduaBPJxap7ExEAqKgjzEzY25FGTs/bOBPG1R2JiLxp6CPg6uH96B/1wIeXLFNZWciEncK+jjIzDBun1DKW3tqeW27ys5EJL4U9HHy6Qv70K1jDgtWquxMROIr4qA3s2Fm9oKZ1ZrZVjOb0mbfVDPbaGZHzexdM7s+0uMlq5NlZ3/ZXM3GfSo7E5H4iSjozSwLeBp4BugCzAGWm9lgM+sNLAe+BBQBdwGPmVn3yEZOXrdc3J+CnEwWqexMROIo0iv6oUAv4Hvu3uLuLwCrgOlAH+Cwu//BQ34H1ANlER4zaRUX5DDton785q0P2KuyMxGJk0iD3k6zbQRQCWw0s2vNLDO8bNMIrG/3iczmmFmlmVVWV1dHOFbimj1hIACLX1LZmYjER6RBvwmoAu4ys2wzuwqoAArcvQV4GHiMUMA/Bsx19/r2nsjdF7p7ubuXl5SURDhW4updnM+1F/TiF2t3c7ihKehxRCQNRBT07t4MXA9MBvYDdwCPA3vM7BPAfwKXAzmEfgD8zMxGR3LMVDCnIlx29prKzkQk9iK+68bd17t7hbt3dfergVJgDTAaWOnule7e6u5rgdXAJyI9ZrIb2qOIy4eUsHTVTpWdiUjMReP2ylFmlmdmBWZ2J9ATWAasBSacvII3szHABE6zRp9u5lWU8WF9E79ap7IzEYmtaLxhajqwj9Ba/RXAle7e6O4rgPuAX5nZUeBJ4Nvu/qcoHDPpjRvYhQv6FrPoJZWdiUhsRWPp5i537+zuHd19krtvbbPvR+4+yN0L3b3U3f8r0uOlCjNj3sRSdn3YwB9VdiYiMaQKhABdNbwHA7t1UNmZiMSUgj5AJ8vO1u+p5dXtHwY9joikKAV9wG74aO9Q2dkK1SKISGwo6AOWl53JrMsGsuI9lZ2JSGwo6BPALeP60yEnkwUrVGEsItGnoE8AnQqy+ezYfvx2/T721DQEPY6IpBgFfYK4dfxADFj8ssrORCS6FPQJoldxPteO7sUv1rxPTb3KzkQkehT0CWTuxDKONavsTESiS0GfQIb0KOTjQ7uz7BWVnYlI9CjoE8zciaV8WN/EEyo7E5EoUdAnmLEDuzC6bzGLVqrsTESiQ0GfYMyMeRWl7D7UwLPvqOxMRCKnoE9AV36kB6UqOxORKFHQJ6DMDOP2iaW8vbeWV7ep7ExEIqOgT1BTxvSmW8dcHlypsjMRiYyCPkGFys4GsPK9at79QGVnInLuFPQJ7JaLw2VnK1V2JiLnTkGfwDrlZ3PTuH48s34f7x9S2ZmInBsFfYK7dfxAMkxlZyJy7hT0Ca5np3yuG92bX65V2ZmInBsFfRKYM7GUY80tPPyqys5E5Owp6JPA4PMKuWJodx56dSfHmlR2JiJnR0GfJOZWlHGovolfrXs/6FFEJMko6JPERQM689F+xSx6aQcnWlqDHkdEkoiCPkmYGXMryth9qIE/qOxMRM6Cgj6JXDnsPEq7dWDBSpWdiciZU9AnkYwMY87EUt7Ze4RXVHYmImdIQZ9kpny0NyWFuTy4QrUIInJmFPRJJjcrk1svG8hLWw7yzt7aoMcRkSSgoE9CN43rR8fcLBaqwlhEzoCCPgmdLDv73dsqOxORf0xBn6RuvSxUdvbDF7YwdcGrVB09HvRIcVN15HjanbNIJBT0SapHpzyuH92bJ9ftYe3OQzzw3JagR4qbB57fknbnLBIJS8T7scvLy72ysjLoMRLakHv+QOOJv32HrBmMH9QtgIli7+WtB2nv2zU3K4PN8yfFfyCRBGNm69y9/NTtWVF44mHAj4ELgWrgLnd/ysxuBha0eWgGkA+Uu/u6SI+b7l66+2PM//1Gfr9+HydaHTMozs+md+d86hpPBD1eTAzvVcTemmPUNDQDkJOZwaSRPfjq5GEBTyaS2CIKejPLAp4GHgSuBCqA35rZGHd/FHi0zWM/B3wNeD2SY0pI96I8CnOzaHEnNyuDppZWJo/syfwpI4MeLaa++tTbPLZmNzg0tbRyvLmF7oV5QY8lktAiXaMfCvQCvufuLe7+ArAKmN7OY2cCD3sirhUlqYN1jdw8rj9PfeEybh7Xn+q6xqBHirmT5/zIbWPpmJvJ8xur2Lz/aNBjiSS0iNbozWwk8CpQeDLAzezPQJ27T2nzuP7AdmCQu7f7mXhmNgeYA9CvX78Ld+3Sh2zI3/f+oQZu/OkrADz5+Uvp26Ug4IlEgnW6NfpIr+g3AVXAXWaWbWZXEVq+OfVv3AzgpdOFPIC7L3T3cncvLykpiXAsSQd9uxTwyOxxHG9uYfri1VQfTf1/0Yici4iC3t2bgeuBycB+4A7gcWDPKQ+dATwUybFE2jOkRyFLZ43lwJFGZi5Zw5HjzUGPJJJwIr6P3t3Xu3uFu3d196uBUmDNyf1mdhmhdfxfRXoskfZc2L8zD06/kC1VR7ltWSXHm/VxiyJtRRz0ZjbKzPLMrMDM7gR6AsvaPGQm8KS76xUziZmKwSX899TRrN11iP/z6Os061O4RP4qGu+MnQ7sI7RWfwVwpbs3AphZHjAVLdtIHHzqgl5887oRPL+pirt/tZ7WVt3gJQJReMOUu98F3HWafceB4kiPIXKmpl/cn8P1TfzXn9+juCCbe//pI5hZ0GOJBCrioBdJNF/8+CBqGppZsmoHXQpy+Jcrzg96JJFAKegl5ZgZ90wexuGG8JV9hxymX9w/6LFEAqOgl5SUkWF859OjOHK8mXuffodO+dlce0GvoMcSCYRqiiVlZWdm8KObPspFA7rwpV++yV82VwU9kkggFPSS0vKyM/nZzHIGn1fIvOXrWLfrUNAjicSdgl5SXlFeNg/dOpYeRXnMWrqWTfuPBD2SSFwp6CUtlBTm8sjsceTnZDJj8Rp2f6jP2pX0oaCXtHGyBK2ppZXpS1brM2clbSjoJa0MPq+QpZ+7iOqjjcxYvIbaYypBk9SnoJe0M6ZfZxZMv5Bt1XXMXraWY00qQZPUpqCXtDTh/BK+/5kxrNtdwxceXacSNElpCnpJW5NH9eRb14/kxc3V3PnEWypBk5Sld8ZKWrtpXD9qGpr47h83U5yfzX3XDlcJmqQcBb2kvS9cXkZNfRM/e3kHnTvk8K+fGBz0SCJRpaCXtGdmfHXyMA4fa+b7z22hc0EOMy8dEPRYIlGjoBchFPb/ccNIao818/XfbKC4IJvrRvcOeiyRqNCLsSJhWZkZ/PCzYxg3sAt3PP4WL25SCZqkBgW9SBsnS9CG9izk84+uY+1OlaBJ8lPQi5yiMC+bZbPG0qtTPrcuW8vGfSpBk+SmoBdpR7eOuTw8eywdc7OYvngNuz6sD3okkXOmoBc5jT6dC3hk9lhaWlu5ZfFqqo6oBE2Sk4Je5O8Y1L2QpbPG8mFdE9MXr6G2QSVoknwU9CL/wOi+xSycXs6Og/Xc+tBaGppOBD2SyFlR0IucgfHnd+MH00bzxu4aPr/8dZpOqARNkoeCXuQMTRrZk29PGcmK96q5QyVokkT0zliRszBtbD9qGpr5zrObKM7P5pvXqQRNEp+CXuQsff7yMg43NLFg5XY6d8jhS1eqBE0Sm4Je5Bx8ZdJQahqaeOD5LXQuyGbWZQODHknktBT0IufAzPj2lFAJ2jd++y7FBdlMGdMn6LFE2qUXY0XOUVZmBj+YNoZLy7py5xPreX7jgaBHEmmXgl4kAnnZmSycUc7wXkV84dHXWbNDJWiSeBT0IhHqmJvF0s9dRO/O+cxetpYNH9QGPZLI/6KgF4mCrh1zWT57HIV5WcxcsoYdB1WCJolDQS8SJb2K83l49jhaHaYvXs3+WpWgSWKIOOjNbJiZvWBmtWa21cymtNlXYGY/MbOD4f0rIz2eSCIb1L0jy2ZdRE19EzOWrOZwQ1PQI4lEFvRmlgU8DTwDdAHmAMvN7OQ7SBaGtw8L//5vkRxPJBmM6lPMohnl7DzYwKxlKkGT4EV6RT8U6AV8z91b3P0FYBUw3cyGANcCc9y9Orx/XYTHE0kKlw7qxgOfHcNb7x9m7iPrVIImgYo06Nsr+TBgBDAO2AV8I7x087aZ3XjaJzKbY2aVZlZZXV0d4VgiwbtmRA/+44ZRvLTlIF96/E1aVIImAYk06DcBVcBdZpZtZlcBFUAB0IdQ4NcSuur/IvCQmQ1r74ncfaG7l7t7eUlJSYRjiSSGqRf15f99cijPrN/HvU+/g7vCXuIvogoEd282s+uBHwJfBiqBx4FG4BjQDMx39xPACjN7EbgK2BjR1CJJZM7EMg7VN/Pgim106ZDDHVcNCXokSTMRd924+3pCV/EAmNkrwEPA1kifWyRVfPmaIRxuaOKHL2yluCCH2eNVgibxE43bK0eZWV74Vso7gZ7AMmAlsBv4dzPLMrPLgMuBP0Z6TJFkY2Z8a8pIJo3owf3PvMuT6/YEPZKkkWi8YWo6sI/QWv0VwJXu3ujuzcB1wCcJrdMvAma4+6YoHFMk6WRmGN+fNprLBnXl7ifX8+d3VYIm8WGJ+OJQeXm5V1ZWBj2GSEzUNZ7g5kWvsXH/UR6+dSwXl3YNeiRJEWa2zt3LT92uCgSROOuYm8XSWWPp16WA2x+q5J29KkGT2FLQiwSgS4ccHpk9lqL8bGYuWcP26rqgR5IUpqAXCUjPTvk8MnssANMXr2Ff7bGAJ5JUpaAXCVBpSUceunUstceambF4DTX1KkGT6FPQiwRsRO9OLJpRzq5DoRK0+kaVoEl0KehFEsAlZV350WfH8PbeWuYtX0fjiZagR5IUoqAXSRBXDe/Bd24Ml6D98i2VoEnURFyBICLR8+kL+3C4oYn5v9tIUX42354yArP2SmJFzpyCXiTB3DahlJqGJn784jY6F2Rz9zVDgx5JkpyWbkQS0J1XDeGmcf34yV+2sWjldqqOHGfqglepOpo+n0Obbuccy/NV0IskIDPj/utGMHlkT771+43831+8wdqdh3jguS1BjxY3Dzy/Ja3OOZbnq64bkQQ25J4/0NjOxxAaoQ8iT0Vbq+poL5VS9ZxPd765WRlsnj/prJ7rdF03WqMXSWAv3f0xvvHbDTy74QAtrU6GQY9OeQzrUUhudmbQ48VE3y75bNp3lP1HjtPqpPw5n3q+uVkZXDOiB1+d3O6H8Z0TBb1IAutelEdxQQ6t7uRmZdDU0srHh3Rn/pSRQY8WU1996m0eW7M7bc751PMtzM2ie2Fe1J5fQS+S4A7WNXLzuP7cNLYfj63ZTXUavDiZbucc6/PVGr2ISIpQH72ISJpS0IuIpDgFvYhIilPQi4ikOAW9iEiKU9CLiKS4hLy90syqgV1Bz3GWugEHgx4iznTO6UHnnDz6u3vJqRsTMuiTkZlVtnf/airTOacHnXPy09KNiEiKU9CLiKQ4BX30LAx6gADonNODzjnJaY1eRCTF6YpeRCTFKehFRFKcgl5EJMUp6CNkZrlmttjMdpnZUTN7w8zO7oMek5SZnW9mx81sedCzxIOZTTOzjWZWb2bbzGxC0DPFkpkNMLPfm1mNme03sx+ZWUp9WJGZfdHMKs2s0cyWnbLvCjPbZGYNZvaimfUPaMyIKegjlwW8D1QAnYCvAY+b2YAAZ4qXHwNrgx4iHszsSuA7wCygEJgIbA90qNj7CVAF9ARGE/oe/0KgE0XfB8B8YEnbjWbWDfg1ob/PXYBK4Jdxny5KUuqncxDcvR64r82mZ8xsB3AhsDOImeLBzKYBh4FXgEEBjxMP3wC+6e6vhb/eG+QwcTIQ+JG7Hwf2m9mzwPCAZ4oqd/81gJmVA33a7LoB2ODuT4T33wccNLOh7r4p7oNGSFf0UWZm5wGDgQ1BzxIrZlYEfBO4I+hZ4sHMMoFyoMTMtprZnvAyRn7Qs8XYD4BpZlZgZr2BScCzAc8UL8OBt05+Eb6g20aS/qBT0EeRmWUDjwIPJeNP/bNwP7DY3d8PepA4OQ/IBj4NTCC0jDEGuCfIoeJgBaFgOwLsIbR88T+BThQ/HYHaU7bVElq2SzoK+igxswzgEaAJ+GLA48SMmY0GPgF8L+hZ4uhY+Pcfuvs+dz8I/DfwyQBniqnw9/MfCa1TdyDU5tiZ0OsU6aAOKDplWxFwNIBZIqagjwIzM2AxoSu/G929OeCRYulyYACw28z2A3cCN5rZ60EOFUvuXkPoijad3kbeBehLaI2+0d0/BJaSwj/cTrEBuODkF2bWASgjSZdkFfTR8VNgGPApdz/2jx6c5BYS+oYfHf71IPA74Oogh4qDpcC/mFl3M+sM/CvwTMAzxUz4Xy07gM+bWZaZFQMzabNunQrC55YHZAKZZpYXvoX0KWCEmd0Y3n8vsD5Zl2QV9BEK31s7l1Do7TezuvCvmwMeLSbcvcHd95/8ReifuMfdvTro2WLsfkK3kr4HbATeAL4V6ESxdwNwDVANbAVOAP8W6ETRdw+hpbmvALeE/3xP+Pv5RkL/j2uAccC0oIaMlErNRERSnK7oRURSnIJeRCTFKehFRFKcgl5EJMUp6EVEUpyCXkQkxSnoRURSnIJeRCTF/X9ze0Spezx4KAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[100.0,\n",
       " 97.05882352941177,\n",
       " 97.05882352941177,\n",
       " 95.58823529411765,\n",
       " 95.58823529411765,\n",
       " 95.58823529411765]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choosing k value\n",
    "np.random.seed(53)\n",
    "k_values = np.arange(1, 12, 2)\n",
    "results_acc = []\n",
    "\n",
    "for k in k_values:\n",
    "    print(f'k = {k}')\n",
    "    knn_clf = KNeighborsClassifier(n_neighbors=k).fit(X4, y4.ravel())\n",
    "    y_pred = knn_clf.predict(X4)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(y4.ravel(), y_pred)*100\n",
    "    results_acc.append(acc)\n",
    "\n",
    "plt.figure\n",
    "plt.plot(k_values, results_acc, '-*')\n",
    "plt.show()\n",
    "results_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.05882352941177"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "np.random.seed(32)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=2)\n",
    "model4 = knn_clf.fit(X4, y4.ravel())\n",
    "y4_pred = model4.predict(X4) \n",
    "accuracy_score(y4, y4_pred) * 100 #training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  0,  1],\n",
       "       [ 0, 20,  1],\n",
       "       [ 0,  0, 26]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y4_pred, y4) #7 missclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 2, 2, 1, 1, 2, 5, 2, 2, 2, 2, 1, 2, 2, 5, 1, 5, 2, 1, 1, 5, 5,\n",
       "       5, 5, 1, 1, 2, 1, 5, 5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(32)\n",
    "y4_test_pred = model4.predict(test4)\n",
    "y4_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 9, 2: 11, 5: 10}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y4_test_pred, return_counts=True)\n",
    "dict(zip(unique, counts)) #how many in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy score 0.9428571428571428 (pvalue : 0.009900990099009901)\n"
     ]
    }
   ],
   "source": [
    "# Cross- validation and permutation\n",
    "np.random.seed(32)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=2)\n",
    "score, permutation_scores, pvalue = permutation_test_score(\n",
    "    knn_clf, X4, y4.ravel(), scoring=\"accuracy\", cv=10, n_permutations=100, n_jobs=1)\n",
    "print(\"Test accuracy score %s (pvalue : %s)\" % (score, pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set “train-3.csv” was used to create a k- nearest neighbours (KNN) classification model to classify speaker examples as belonging to one of three different languages. The data set contains the same variables as discussed in Question 3.  Cross- validation was used to find the optimal value of k, the number of nearest neighbours. Although k=1 resulted in 100% training accuracy, k=2 was chosen to avoid overfitting the model and to make the model more generalisable. \n",
    "\n",
    "The scaled training data was fit to a KNN classification model and the class labels of the training data were predicted. The resulting training accuracy and proportion of correct class predictions was 97.06%, with 2 misclassifications. A confusion matrix showed that the distribution of correct predictions was relatively evenly spread, with 20 observations in the belonging to the first two classes, and 26 to the third class.\n",
    "\n",
    "The scaled test data was applied to the trained model to predict class labels for the data, with 9 observations predicted to belong to class “1”, 11 to class “2” and 10 to class “5”, indicating a fairly even distribution.\n",
    "\n",
    " As above, to evaluate the test accuracy of such a KNN classification model, 10- fold cross validation using the training data. The significance of the classification accuracy score was computed with 100 permutations. The test accuracy was determined to be 94.29% and the p-value to be 0.009, indicating significance the 0.01 level and model performance greater than chance level. It can be seen that the classification accuracy for the k- nearest neighbours classifier is slightly better than for the support vector classifier. Due to the high classification accuracy obtained through cross-validation, as well as its performance better than chance level, the k- nearest neighbours classification model may be considered to be the better approach.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
